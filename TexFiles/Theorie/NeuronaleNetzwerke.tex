\newpage
\section{Neuronale Netzwerke}
\label{sec:NN}
Dieser Abschnitt widmet sich den Konzepten künstlicher neuronaler Netze. Als Hauptquelle dient der Artikel \textit{Building a neural network from scratch} von David Selby \cite{SelbyNN} sowie die Vorlesung \textit{Artificial Intelligence} von Dr. Stroetmann \cite{stroetmann}.

~\newline Neuronale Netze erhielten ihren Namen, da man zu Beginn der Forschung dachte, dass das menschliche Gehirn wie ein \textit{computational graph} funktionierte. Diese These wurde biologisch weitgehend widerlegt und deswegen werden die neuronalen Netze der Informatik mit dem Zusatz \textit{künstlich} markiert. 

~\newline Im Folgenden wird zunächst der Aufbau des Modells und anschießend das Training vorgestellt.
\subsection{Modell künstlicher neuronaler Netze}
\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.4\linewidth]{Bilder/singleNeuron}
		\caption[Einzelnes Neuron: \url{
			http://caisplusplus.usc.edu/blog/curriculum/lesson4}]{Einzelnes Neuron}
		\label{fig:Neuron}
	\end{center}
\end{figure}

Grundidee des Modells bildet das Konzept eines \textbf{Neurons}. Dieses erhält Eingabewerte, und sobald ein gewisser Schwellwert erreicht wurde, \textit{feuert} es sein Signal ab, um andere Neuronen zu kontaktieren oder Handlungen hervorzurufen. 

Im Rahmen der Informatik äußert sich diese Neuronen-Logik durch eine Aktivierungsfunktion, die üblicherweise ohne Bedingung eine Ausgabe erzeugt. Dies zeigt Abbildung \ref{fig:Neuron}.
\begin{figure}[h]
\begin{center}
	\includegraphics[width=0.9\linewidth]{Bilder/petry19}
	\caption[Modell eines neuronalen Netzwerkes: \url{
		http://www.jurpc.de/jurpc/show?id=19990187}]{Modell eines neuronalen Netzwerkes}
	\label{fig:NN-Modell}
\end{center}
\end{figure}
~\newline Diese Neuronen werden zu einen Graphen, und sind in \textit{Schichten} (engl. Layer) angeordnet. 

Jedes Neuron einer Schicht erhält Eingaben von jedem Neuron der vorhergehenden\footnote{In diesem Fall spricht man von einem \textbf{vollvermaschten} neuronalen Netz. Es ist möglich, andere Formen der Verknüpfung oder Filterkriterien einzustellen.}. Diese Eingaben werden zusätzlich gewichtet. 

~\newline Die Eingabe in das neuronale Netzwerk erfolgt über den Inputlayer, welcher keine Aktivierungsfunktion hat. Die Ausgabe des neuronalen Netzwerkes erfolgt in der sog. Ausgabeschicht, welche je nach Art des Netzes einen (für Regressionen), zwei (für binäre Klassifikationen) oder $n$ (für n-Klassen Multiklassifikation) Knoten besitzt.

Die Neuronen zur Berechnung befinden sich in den \textit{Hidden Layers}. Gibt es mehr als einen Hidden-Layer spricht man von einem \textit{deep neuronal network}\footnote{Grund hierfür ist die Funktion der tieferen Schichten - Anstatt nur die Eingabe zu gewichten, werden hier weitere Features erkannt bzw. erzeugt, welche nur für den Algorithmus erkennbar sind.}. Dieses vollständige Modell zeigt Abbildung \ref{fig:NN-Modell}.

~\newline Um das Modell zu trainieren, muss ebenfalls der Fehler der Schätzung minimiert werden. Hierbei werden die Konzepte der linearen und logistischen Regression verwendet, jedoch mit dem Zusatz, das anstatt eines einzelnen Gewichtsvektors eine Gewichtsmatrix angepasst werden muss. 
\subsection{Forward Propagation}
Unter der \textit{Vorwärtsausbreitung} versteht man den Algorithmus, welcher einen Eingabevektor durch alle Gewichtsvektoren und Schichten transfomiert. 

Dieser \textit{Feed Forward}-Prozess kann sowohl iterativ über alle Vektoren erfolgen, oder zusammengefasst als Matrizenoperation.
\subsection{Backward Propagation}
Die \textit{Rückwärtsausrichtung} bezeichnet den Algorithmus, welcher die Gewichte anhand des gemessenen Fehlers nachjustiert. 

Hierbei wird in gleichem Maße wie in der logistischen Regression vorgegangen, mit dem Unterschied, dass die Gewichte in einer Matrix vorliegen. 

Die Eigenschaften der Aktivierungsfunktionen bezüglich ihrer Ableitung finden hier im besonderen Maße Anwendung, denn um von der Ausgabeschicht auf den letzten Hidden-Layer nachzujustieren, benötigt man die erste Ableitung. Um auf den nächsten Hidden Layer Einfluss zu nehmen, muss die zweite Ableitung gebildet werden (usw.). Eine mathematische Ausarbeitung findet sich unter \cite{colah} \textit{Computational Victories}.
\subsection{Training}
Das Training bezeichnet den (iterativen) Prozess, mit den vorliegenden Trainingsdaten zunächst Forward-Propagation durchzuführen, um anschließend mittels Backward Propagation das Netz auszurichten. 

~\newline Der Umfang dieses Trainings bleibt dem Anwender überlassen. Es ist möglich, bessere Ergebnisse zu erzielen, indem man mit denselben Daten häufiger trainiert. Einen solchen wiederholten Trainingsdurchlauf nennt man eine \textbf{Epoche}. 
\subsection{Bewertung des neuronalen Netzes}
Die Bewertung des neuronalen Netzes erfolgt, je nach Art des Ergebnisses, analog wie die der linearen oder logistischen Regression. 
\subsection{Einflüsse auf den Trainingserfolg}
Zum Abschluss dieses Abschnittes werden zusammenfassend die \textit{Stellschrauben} vorgestellt, anhand derer Änderungen des Trainingserfolges erzielt werden können. 

\begin{itemize}
	\item \textbf{Netzaufbau und Struktur:} Die Anzahl der Knoten, Schichten, und Einstellungen zur Verknüpfung können variiert werden.
	\item \textbf{Optimierungsfunktion:} Hierbei kann der grundlegende Algorithmus (Gradient Descent oder Stochastic Gradient Descent), sowie Trainingsparameter (Lerngeschwindigkeit, Beschleunigung, Verfall) gewählt werden.
	\item \textbf{Aktivierungsfunktion der Neuronen}
	\item \textbf{Menge der Trainingsdaten:} Eine größere Menge an Trainingsdaten hilft maßgeblich, den Sachverhalt besser erfassen zu können. Auch sind große Datenmengen notwendig, um bei komplexeren Netzen \textit{overfitting} zu vermeiden.
	\item \textbf{Anzahl der Features} 
	\item \textbf{Anzahl der Epochen und Iterationen}
	
\end{itemize}

Konkrete Anwendungen dieser Parameter und die damit erzielten Ergebnisse finden sich im Kapitel \ref{cha:Experiment} dieser Arbeit.
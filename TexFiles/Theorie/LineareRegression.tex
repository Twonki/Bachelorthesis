\section{Lineare Regression}
\label{sec:LineareRegression}
\todo{Stroetmann und oder Formelsammlungen zitieren}
Als erster Machine-Learning-Algorithmus soll die Lineare Regression vorgestellt werden. 

Auch wenn lineare Regression nicht mehr Bestandteil aktueller Forschung ist, sind viele Konzepte für weitere Erklärungen nützlich. Zudem können mithilfe linearer Regression sehr gute Ergebnisse erzielt werden.
\subsection{Konzept und Ziele linearer Regression}
Als Beispiel für die einfache lineare Regression dient uns das Abschätzen des Bremsweges von PKWs. Hierfür benötigen wir eine Tabelle der Gestalt

\begin{center}
	\label{tab:Bremsweg}
	\begin{tabular}{|p{0.2\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|}
		\hline
		Geschwindigkeit in km/h & Gewicht in kg & Bremsweg in m  \\ \hline
		50& 1500 & 20 \\ \hline
		60& 1400 & 30 \\ \hline
		90& 1000 & 60 \\ \hline
		...& ... & ... \\ \hline
	\end{tabular}
\end{center}
~\newline
Es ist hierbei offensichtlich, das diese Messwerte zusammenhängen - lediglich die zugrunde liegende Formel ist uns Unbekannt. 

Mithilfe linearer Regression wollen wir eine modellhafte Formel finden, die das beste Ergebnis anhand unserer Daten liefert. Die Werte müssen für 
\subsection{Einfache Lineare Regression}
Zunächst gehen wir zur Vereinfachung davon aus, dass der Bremsweg lediglich von der Geschwindigkeit abhängt. Bezeichnen wir $x_i$ als die Geschwindigkeit des $i$-ten Datensatzes der Tabelle \ref{tab:Bremsweg} und $y_i$ als den zugehörigen Bremsweg, kann man ein lineares Modell der Form 
\begin{equation}
	y_i := \vartheta_1 \cdot x_i + \vartheta_0 
\end{equation}

herleiten. Wir wollen $\vartheta_1$ und $\vartheta_0$ so berechnen,dass der \textbf{Mean-Squared-Error} minimal ist. 
\begin{equation}
\label{eq:mse}
\mathtt{MSE}(\vartheta_0, \vartheta_1) := \frac{1}{m-1} \cdot \sum\limits_{i=1}^m \bigl(\vartheta_1 \cdot x_i + \vartheta_0 - y_i\bigr)^2
\end{equation}

Die optimalen Ergebnisse des MSE liefern die Variablen:

\begin{equation}
\label{eq:theta0}
\vartheta_1 = r_{x,y} \cdot \frac{s_y}{s_x} \quad \mbox{und} \quad \vartheta_0 = \bar{\mathbf{y}} - \vartheta_1 \cdot \bar{\mathbf{x}}.
\end{equation}

Wobei $\mathbf{x}$ und $\mathbf{y}$ das arithmetische Mittel der beiden Variablen darstellt, sowie $s_x$ und $s_y$ die Standart-Abweichungen. bei $r_{x,y}$ handelt es sich um den \textbf{Pearson-Korrelationskoeffizienten}. 

~\newline Nach der Berechnung unserer \textit{Gewichte} besitzen wir ein Modell, welches für jeden beliebigen Geschwindigkeitswert den Bremsweg berechnet. 

Dennoch können wir davon ausgehen, das ein lineares Modell für komplexere Sachverhalte keine zufriedenstellenden Ergebnisse liefert. Deswegen wird nun die lineare Regression unter Berücksichtigung mehrerer unabhängiger Variablen behandelt. 
\subsection{Allgemeine Lineare Regression}
Das Prinzip der allgemeinen linearen Regression ist das Gleiche: Wir suchen eine Funktion, welche uns den abhängigen Wert schätzt. Diese Funktion hat im Allgemeineren die Form  $F:\mathbb{R}^m -> \mathbb{R}^1$, und bildet ein $m$-Eigenschaften umfassendes Tupel $x_i$ auf einen Wert $y_i$ ab. 

Bezogen auf unser Beispiel \ref{tab:Bremsweg} haben wir ein 2-Tupel $x_i$ der Form <Geschwindigkeit,Gewicht> und weiterhin einen dazugehörigen Bremsweg $y_i$. Wir suchen eine Funktion $F(x_i) \approx y_i$. 

~\newline Diese Funktion können wir ebenfalls durch ein lineares Modell ausdrücken. Sie hat die Gestalt: 

\begin{equation}
	F(x_i)= \vartheta_2 \cdot x^2_i + \vartheta_1 \cdot x^1_i + \vartheta_0 \cdot x^0_i
\end{equation}
Wobei $x^n_i$ die $n$-te Komponente des $i$-ten Elementes darstellt. $x^0$ ist eine Erweiterung um den Bias. Interpretiert man die Tupel (erweitert um den Bias) als transponierten Vektor $x^T$ so kann man die allgemeine Formel für größere Tupel zusammenfassen als:

\begin{equation}
	F(x) = \sum_{n=0}^{m} x^n \cdot \vartheta_n = x^T \cdot \vec{w}
\end{equation}
wobei $ \vec{w} = \begin{pmatrix}\vartheta_0 \\\ ... \\\ \vartheta_n \end{pmatrix}$ der Gewichtsvektor ist. Nun lässt sich unsere Funktion um den Fehler zu berechnen definieren als:

\begin{equation}
\label{eq:squared-error-1}
\mathtt{MSE} := \frac{1}{m-1} \cdot \sum\limits_{i=1}^{m} \Bigl(F\bigl(\mathbf{x}^{(i)}\bigr) - y^{(i)}\Bigr)^2=\frac{1}{m-1} \cdot \sum\limits_{i=1}^m \Bigl(\bigl(\mathbf{x}^{(i)})^\top \cdot \vec{w}  - y^{(i)}\Bigr)^2
\end{equation}

\todo{Vektoren summieren und normalengleichung}
\subsection{Bewertung der Linearen Regression}
Wie berechne ich die statistische Signifikanz meines Linearen Modells?
\section{Lineare Regression}
\label{sec:LineareRegression}
Als erster Machine-Learning-Algorithmus soll die lineare Regression vorgestellt werden. 

Auch wenn lineare Regression nicht mehr Bestandteil aktueller Forschung ist, sind sie für weitere Erklärungen hilfreich, da Neuronale Netze mit Regression sich ebenfalls auf dieses Verfahren stützen. 

~\newline Zudem können mithilfe einfacher linearer Regression bereits sehr gute Ergebnisse erzielt werden.
\subsection{Konzept und Ziele linearer Regression}
Als Beispiel für die einfache lineare Regression dient das Abschätzen des Bremsweges von PKWs. Hierfür benötigen man eine Tabelle der Gestalt

\begin{center}
	\label{tab:Bremsweg}
	\begin{tabular}{|p{0.2\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|}
		\hline
		Geschwindigkeit in km/h & Gewicht in kg & Bremsweg in m  \\ \hline
		50& 1500 & 20 \\ \hline
		60& 1400 & 30 \\ \hline
		90& 1000 & 60 \\ \hline
		...& ... & ... \\ \hline
	\end{tabular}
\end{center}
~\newline
Es ist hierbei offensichtlich, das diese Messwerte zusammenhängen - lediglich die zugrunde liegende Formel ist unbekannt. 

Mithilfe linearer Regression wollen wir eine modellhafte Formel finden, die das beste Ergebnis anhand unserer Daten liefert.
\subsection{Einfache Lineare Regression}
Zunächst geht man zur Vereinfachung davon aus, dass der Bremsweg lediglich von der Geschwindigkeit abhängt. Bezeichnet man $x_i$ als die Geschwindigkeit des $i$-ten Datensatzes der Tabelle \ref{tab:Bremsweg} und $y_i$ als den zugehörigen Bremsweg, kann man ein lineares Modell der Form 
\begin{equation}
	y_i := \vartheta_1 \cdot x_i + \vartheta_0 
\end{equation}

herleiten.Zu berechnen ist $\vartheta_1$ und $\vartheta_0$ so, dass der \textbf{Mean-Squared-Error} minimal wird. 
\begin{equation}
\label{eq:mse}
\mathtt{MSE}(\vartheta_0, \vartheta_1) := \frac{1}{m-1} \cdot \sum\limits_{i=1}^m \bigl(\vartheta_1 \cdot x_i + \vartheta_0 - y_i\bigr)^2
\end{equation}

Die optimalen Ergebnisse des MSE liefern die Variablen:

\begin{equation}
\label{eq:theta0}
\vartheta_1 = r_{x,y} \cdot \frac{s_y}{s_x} \quad \mbox{und} \quad \vartheta_0 = \bar{\mathbf{y}} - \vartheta_1 \cdot \bar{\mathbf{x}}.
\end{equation}

Wobei $\mathbf{x}$ und $\mathbf{y}$ das arithmetische Mittel der beiden Variablen darstellt, sowie $s_x$ und $s_y$ die Standardabweichungen. bei $r_{x,y}$ handelt es sich um den \textbf{Pearson-Korrelationskoeffizienten}. 

~\newline Nach der Berechnung der \textit{Gewichte} besitzt man ein Modell, welches für jeden beliebigen Geschwindigkeitswert den Bremsweg berechnet. 

Dennoch kann man davon ausgehen, das ein lineares Modell für komplexere Sachverhalte keine zufriedenstellenden Ergebnisse liefert. Deswegen wird nun die lineare Regression unter Berücksichtigung mehrerer unabhängiger Variablen behandelt. 
\subsection{Allgemeine Lineare Regression}
Das Prinzip der allgemeinen linearen Regression ist das Gleiche: Wir suchen eine Funktion, welche uns den abhängigen Wert schätzt. Diese Funktion hat im Allgemeineren die Form  $F:\mathbb{R}^m -> \mathbb{R}^1$, und bildet ein $m$-Eigenschaften umfassendes Tupel $x_i$ auf einen Wert $y_i$ ab. 

Bezogen auf das Beispiel \ref{tab:Bremsweg} hat man ein Tupel $x_i$ der Form <Geschwindigkeit,Gewicht> und weiterhin einen dazugehörigen Bremsweg $y_i$. Wir suchen eine Funktion $F(x_i) \approx y_i$. 

~\newline Diese Funktion können wir ebenfalls durch ein lineares Modell ausdrücken. Sie hat die Gestalt: 

\begin{equation}
	F(x_i)= \vartheta_2 \cdot x^2_i + \vartheta_1 \cdot x^1_i + \vartheta_0 \cdot x^0_i
\end{equation}
Wobei $x^n_i$ die $n$-te Komponente des $i$-ten Elementes darstellt. $x^0$ ist eine Erweiterung um den Bias.


~\newline Für dieses Modell, bzw. allgemein für alle Modelle dieser Form kann ebenfalls der MSE berechnet und minimiert werden, um eine optimale Gewichtsmatrix zu erzeugen. Die genauen mathematischen Verfahren hierfür finden sich z.B. im Vorlesungsskript von Prof. Stroetmann \cite{stroetmann} Kapitel 5 Abschnitt 2 und Unterabschnitte.
\subsection{Bewertung der Linearen Regression}
\label{subsec:LinRegAcc}
Um die statistische Aussagekraft des Modells zu bewerten, eignet sich ein \textbf{F-Test} (\cite{stroetmann} S. 86 Absatz 1), dieser ist definiert durch die Formel:
\begin{equation}
\label{eq:F-statistic}
\mathrm{F} = \frac{\mathtt{TSS} - \mathtt{RSS}}{\mathtt{RSS}} \cdot \frac{m - p - 1}{p}
\end{equation}
Die $\mathtt{F}$-Statistik ist Fisher-Snedecor-verteilt mit $p-1$ Freiheitsgraden im Nenner und $m-p$ Freiheitsgraden im Zähler. 
\begin{equation}
\mathtt{TSS} = \sum\limits_{i=1}^m \bigl(y_i - \bar{\mathbf{y}}\bigr)^2 \ \ \
\mathtt{RSS} := \sum\limits_{i=1}^m \bigl(\vartheta_1 \cdot x_i + \vartheta_0 - y_i\bigr)^2
\end{equation}
$\mathtt{TSS}$ wird hierbei als die \textit{Total Sum of Squares} bezeichnet, $\mathtt{RSS}$ für die \textit{Residual Sum of Squares}. (Weiterführend: \cite{stroetmann} S. 77 Absatz 4f). 

~\newline Dieser Test ist dahingehend notwendig, da ein simples Vergleichen der Modell-Schätzung (immer) von den echten Werten abweicht. Innerhalb des F-Testes werden ebenfalls die Varianz der Grundgesamtheit in Relation zur Varianz der Modell-Werte betrachtet, um ein aussagekräftiges Ergebnis zu erzielen.


~\newline \textit{Anmerkung:} Für die Bewertung eines ML-Algorithmus, welcher lediglich eine einzelne Variable vorhersagt, ist ein Test auf den $r^2$-Wert ausreichend. 

Da die Ergebnisse und  Varianz der Test-Werte mit den Ergebnissen und der  Varianz der Bild-Werte verglichen werden, liegt lediglich ein Freiheitsgrad vor.  
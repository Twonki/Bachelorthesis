\section{Klassifizerung}
\label{sec:Klassifizierung}
Nach der linearen Regression soll nun die Klassifizierung vorgestellt werden. Hierfür werden zunächst allgemeine Ziele und ein Beispiel vorgestellt, anschließend wird als ausgewähltes Verfahren die logistische Regression erläutert. 
 
\subsection{Konzept und Ziele von Klassifizierung}
Die Klassifizierung zählt zu den ältesten Anwendungen des Machine-Learning - Ein typisches Beispiel für (überwachte) Klassifizierung ist die Zuordnung einer E-Mail in \textit{Spam} oder \textit{Ham}.

~\newline Im Rahmen der Klassifizerung soll ein Modell erzeugt werden, das anhand der Eigenschaften einer E-Mail (z.B. dem Auftreten des Wortes \textit{Pharmacy} oder \textit{Sex}) feststellt, ob es sich um typische Spam-E-Mails handelt. 

~\newline Das Modell erzeugt einen Wahrscheinlichkeitswert, mit welchem die Klasse angenommen wird (bzw. im Umkehrschluss, wie wahrscheinlich das Gegenereignis ist) und \textit{rät} die entsprechende Klasse. 

~\newline Des Weiteren ist es möglich, eine sog. Multiklassen-Klassifizierung durchzuführen. Hierbei werden mehr als zwei Klassen betrachtet. 
\subsection{Logistische Regression}
\label{subsec:LogRegAcc}
Innerhalb der Logistischen Regression wird ein Modell erzeugt, welches einen Eigenschaftsvektor mit einem Gewichtsvektor multipliziert, und das Ergebnis über eine Aktivierungsfunktion in eine Wahrscheinlichkeit abbildet. 

~\newline Dieses Verfahren ermöglicht uns, anstatt der numerischen Zählung der \textit{Treffer}, die reale Wahrscheinlichkeit zu optimieren, und dahingehend unsere Gewichte \textit{smooth} zu justieren. 

~\newline Das Optimieren des Modells benötigt Trainingsdaten und eine \textbf{Optimierungsfunktion}. Mit jedem Satz der Trainingsdaten wird der Gewichtsvektor dahingehend angepasst, das die resultierende Wahrscheinlichkeit in Richtung des korrekten Ergebnisses verschoben wird. Das Maß in welchem die Gewichte Angepasst werden, wird über die Optimierungsfunktion ermittelt. 

~\newline Im Normalfall wird der Gewichtsvektor mit zufälligen Werten initialisiert. Es ist jedoch möglich, einen bereits bestehenden Vektor zu importieren. 

Ebenso ist es üblich, sowohl Eingabewerte, als auch den Gewichtsvektor zu normieren. Einige Optimierungsfunktionen, wie z.B. \textit{Stochastic Gradient Descent} terminieren schneller für normierte Daten (vgl. \cite{GDQuora} Absatz 2). Grund hierfür sind die Eigenschaften der Loss-Function, die bei einer normierten Eingabe eine \textit{glatte} Oberfläche besitzt, und somit eine bessere Anpassung der Gewichte ermöglicht (Weiterführend \cite{GDBoost} Abschnitt \textit{Gradient Descent} und Abschnitt \textit{Learning Rate} ). 
\subsection{Aktivierungsfunktion}
\label{subsec:Aktivierungsfunktion} 
Bei der Aktivierungsfunktion handelt es sich um eine statistische Verteilungsfunktion. Sie bildet einen Wert auf eine Wahrscheinlichkeit ab. 

~\newline Als übliche Aktivierungsfunktionen werden $tanh$, die Sigmoid-Funktion oder die ReLu-Funktion gewählt. Diese besitzen besondere Eigenschaften innerhalb ihrer Ableitung, was eine Berechnung wesentlich erleichtert (\cite{stroetmann} S95ff \textit{6.3.1 The Sigmoid Function}).

\paragraph{Sigmoid und Tanh}~\newline
Die Sigmoidfunktion ist eine stochastische Verteilungsfunktion und stellt eine Abwandlung der Tangens-Hyperbolicus Funktion dar. 
\begin{equation}
\label{eq:sigmoid}
sig(t) = \dfrac{1}{1+e^{-t}}=\dfrac{e^t}{1+ e^t}= \frac{1}{2} \cdot ( 1 + tanh \frac{t}{2})
\end{equation}
Die Range der Sigmoidfunktion ist $[0,1]$ und eignet sich insofern explizit für die binäre Klassifizierung, um die Wahrscheinlichkeit der Klasse zu ermitteln. 

Die Sigmoidfunktion und Tanh sind in Abbildung \ref{fig:SigTanh} zu sehen. 

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.65\linewidth]{Bilder/sigmoidtanhplot}
		\caption[Sigmoid und Tanh: \url{
			https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6}]{Sigmoid und Tanh}
		\label{fig:SigTanh}
	\end{center}
\end{figure}  

~\newline Die Tangens-Hyperbolicus Funktion unterscheidet sich dahingehend, das sie auf negative Eingaben stark negativ einschlägt, sowie auf Null-Eingaben ebenfalls Null liefert. 

~\newline Die Ableitung der Sigmoidfunktion in Formel \ref{eq:sigmoidderivate} ist dahingehend besonders, da Sie sich ebenfalls auf die Sigmoidfunktion beruft. 

Nach dem einmaligen Berechnen der Sigmoidfunktion für einen Wert t, lassen sich in einfachen Operationen alle Ableitungen bestimmen. 

\begin{equation}
	\label{eq:sigmoidderivate}
	\dfrac{d \ sig(t)}{d \ t} = sig(t) \cdot (1 - sig(t))
\end{equation}  
\paragraph{Rectified Linear Unit}~\newline
Die Rectified Linear Unit Funktion, kurz \textbf{ReLU} ist eine der am weitesten verwendeten Aktivierungsfunktionen in neuronalen Netzen und ist definiert als: 
\begin{equation}
\label{eq:ReLU}
ReLU(t) := max(0,t)
\end{equation}

Der größte Kritikpunkt an der ReLU-Funktion ist, dass negative Eingabewerte stets als Null gewertet werden, was die Möglichkeiten des Modells von den Daten zu \textit{lernen} stark einschränkt (vgl. \cite{sigmoid} Abschnitt 3 Absatz 5). 
\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.5\linewidth]{Bilder/ReluPlot}
		\caption[Rectified Linear Unit: \url{
			https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/relu_layer.html}]{Plot der ReLU-Funktion}
		\label{fig:ReLUplot}
	\end{center}
\end{figure}  
Es gibt verschiedene Ansätze, diesen Nachteil auszugleichen: ~\newline
\begin{enumerate}
	 \item Bei \textbf{Leaky Rectified Linear Unit} werden negative Werte anstatt als 0 mit einem Bruchteil (ca. $\frac{1}{100}$-stel) ihres Wertes behandelt. 
	\item Bei \textbf{Randomized Rectified Linear Unit} wird dieser Bruchteil zufällig gewählt und ggfs. während der Laufzeit angepasst. 
	\item Bei \textbf{Euler Linear Unit} (\textit{kurz: ELU}) werden negative Werte mit der Funktion $a(e^x-1)$ abgebildet,  wobei $a$ ein gewählter Parameter zwischen 0 und 1 ist.
\end{enumerate}
 
\paragraph{Softmax}~\newline
Die (\textit{echte}) Softmax-Funktion stellt eine logistische Verteilungsfunktion für mehrere Parameter dar. Sie ist definiert als: 

\begin{equation}
	\label{eq:Softmax}
	softmax: \mathtt{R}^K \rightarrow \lbrace z \in \mathtt{R}^K | z_i \geq 0 , \sum_{i=0}^{K} z_i = 1 \rbrace
\end{equation}
\begin{equation}
\label{eq:Softmax2}
	softmax(z)_j = \dfrac{e^{z_j}}{\sum_{k=1}^{K}e^{z_k}}  \ \  j = 1, ... , K
\end{equation}

Die Softmax-Funktion liefert eine Wahrscheinlichkeit für jede Klasse, der ein Trainingsbeispiel angehören kann.Die Wahrscheinlichkeit über alle Klassen ist 1. Sie ist eine Annäherung an die \textit{max}-Funktion.
~\newline \textbf{Softmax als Aktivierungsfunktion} (eines Neurons) mit k-Eingaben ist definiert als:
\begin{equation}
	\label{SoftmaxNeuron}
	softmax(\vec{t})=ln\sum_{i=1}^{K}e^{t_i}
\end{equation}

Die Softmax-Aktivierungsfunktion stellt eine Annäherung an die \textit{max} bzw. ReLU Funktion dar, wie dargestellt in Abbildung \ref{fig:Softmaxplot}.
\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.4\linewidth]{Bilder/Softmaxplot}
		\caption[Softmax: \url{
			https://www.quora.com/Why-is-softmax-activate-function-called-softmax}]{ReLU(Rot) und Softmax(Blau)}
		\label{fig:Softmaxplot}
	\end{center}
\end{figure}  \textit{Softmax} gewinnt dahingehend an Bedeutung, das sie ableitbar ist und somit in den versteckten Schickten des Neuronalen Netzes verwendet und trainiert werden kann (vgl. \cite{Softmax} Abschnitt \textit{What is the Purpose[...]}).

~\newline Die Softmax-Funktion wird in der Ausgabeschicht verwendet für Multiklassen-Klassifizierung. Die Softmax-Aktivierungsfunktion kann innerhalb der versteckten Schichten verwendet werden. 
\subsection{Optimierungsfunktion}
Die Optimierungsfunktion hilft, für eine (unbekannte) Funktion ein Extremum zu finden und wird konkret benötigt, um das Minimum der Fehlerfunktion zu erreichen.  

~\newline Im Rahmen des Machine-Learning verwaltet die Optimierungsfunktion ebenfalls Trainingsparameter, z.B. die Lernrate, eine Beschleunigungs- und Verfallslogik. Ebenso oft Bestandteil sind Funktionen, welche eine zufällige Verschiebung bewirken. Grund hierfür ist eine Schwäche der meisten Optimierungsfunktionen, sich auf ein lokales Extremum einzupendeln.

~\newline Im Folgenden wird der Gradientenanstieg und Optimierungen dessen vorgestellt. Dieser Unterabschnitt stellt im Wesentlichen eine Auswahl und Aufbereitung von Sebastian Ruders Blogeintrag \textit{An overview of gradient descent optimization algorithms} \cite{OptimizationFunction} dar.
\paragraph{Der Gradientenanstieg} Kern des Gradientenanstiegs bildet die iterative Suche nach dem (globalen) Minima der Loss-Funktion. 
\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.6\linewidth]{Bilder/optimization}
		\caption[Loss-Function: \url{
			http://www.ahozer.com/research.html}]{Beispiel einer visualisierten Verlust-Funktion}
		\label{fig:Loss}
	\end{center}
\end{figure}
Einen Beispielhaften Plot einer Verlustfunktion liefert Abbildung \ref{fig:Loss}: Der Verlust ist hierbei auf der Z-Achse eingetragen, die Y- und X-Achse bilden die Gewichte ab. Die vorgestellten Verfahren sind ebenfalls in Räumen höherer Dimension möglich.

Stellt man sich diese Funktion als (hügelige) Landschaft vor (Siehe Abbildung \ref{fig:Loss}), so hilft als Veranschaulichung \textit{the blindfolded hiker}: Ein Wanderer mit Augenbinde steht an einem Berghang und möchte sicher das nächste Tal erreichen. Hierfür findet er zunächst heraus, wo es gerade bergab geht, und tätigt einen vorsichtigen Schritt. Dies wiederholt er so lange, bis er an einer Stelle steht, wo es ausschließlich bergauf geht.

~\newline Um mathematisch \textit{die Hanglage} zu bestimmen benötigt man den Gradienten der Verlustfunktion. Prinzipiell ist es möglich, anhand des Gradienten bereits alle Extrema der Verlustfunktion zu bestimmen - allerdings stellt dies vor den Anwendungen, welche mehrere Tausend Dimensionen aufweisen können, keine technisch umsetzbare Variante dar. 

Vor diesem Hintergrund wird nicht direkt versucht, die \textbf{besten} Gewichte zu bestimmen, sondern lediglich ausgehend von den bisherigen \textbf{bessere}. Hierfür werden die bestehenden Gewichte um einen Bruchteil des Gradienten verändert. Diesen Bruchteil bezeichnet man als \textbf{Lernrate} (engl. learning rate, oft auch Schrittgröße).

~\newline Der Gradient kann hierbei numerisch approximiert werden, oder über die Ableitung gebildet werden. Im Rahmen des Machine Learnings ist die Ableitung üblich - die oben vorgestellten Aktivierungsfunktionen besitzen einfach zu berechnende, bzw. bekannte Ableitungen und erzeugen keine Rechenlast, wie dies bei einer Approximation der Fall ist.

~\newline Die Wahl der Lernrate ist für die Anwendung des Algorithmus entscheidend: Wird bis zu einem Minima iteriert \footnote{Das Verfahren des Gradientenanstiegs kann ggfs. nicht terminieren - in den tatsächlichen Implementierungen wird deswegen immer eine gewisse Anzahl an Iterationen durchgeführt oder andere Kontrollparameter aufgestellt}, so kann die Terminierung des Algorithmus sehr hohe Zeit in Anspruch nehmen. Wird eine gewisse Anzahl an Iterationen durchgeführt, so liegen die Ergebnisse eventuell sehr nah an den zufällig initialisierten Gewichten.

Auf der anderen Seite kann eine hohe Lernrate zu einem \textit{herumspringen} der Gewichte führen, und eine tatsächliche Optimierung verhindern.

~\newline Ausgehend von diesem Grundverfahren, gibt es mehrere Ansätze den Gradientenanstieg zu verbessern:

\paragraph{(Stochastischer Gradientenanstieg}
Im Rahmen des Stochastischen Gradientenanstieg wird das Set der Trainingsdaten in jeder Epoche zufällig Sortiert. 

Der durchgeführte Schritt ist also in einer zufälligen Reihenfolge, und kann dazu führen das sich der Algorithmus nicht dem nächsten Minimum nähert, sondern sich zunächst hiervon entfernt. 

~\newline Dies stellt allerdings einen Vorteil dar: Durch dieses Verhalten wird verhindert, dass lediglich ein lokales Minimum gefunden wird. Allerdings terminiert der Stochastische Gradientenanstieg nicht so schnell, bzw. nicht so nah am reellen Minima wie der normale Gradientenanstieg, zumindest ohne Einbindung weiterer Verbesserungen.

\paragraph{Mini-Batch-Gradientdescent} In dieser Variante wird die Veränderung der Gewichte nicht anhand des Gradientens eines einzelnen Datensatzes, sondern aufgrund einer n-großen Teilmenge (\textit{Batch}) geupdatet. 

~\newline Diese Variation reduziert die auftretende Varianz innerhalb der Schritte und sorgt somit für eine besser absehbare Konvergenz. Zusätzlich wird Batch-Processing von Vektoren innerhalb vieler Prozessoren und Grafikkarten unterstützt. Die \textbf{Batch-Größe} stellt den zweiten wichtigen Hyperparameter dar und wird im Normalfall (aus technischen Gründen) als Zweierpotenz gewählt.   

~\newline Es ist weiterhin üblich, diese Variante mit dem Stochastischen Gradientenanstieg zu kombinieren. In jeder Epoche werden hierbei die Batches zufällig sortiert. Diese Kombination adressiert die Probleme des stochastischen Gradientenverfahrens bezüglich der Terminierung und besitzt trotzdem die Möglichkeit lokale Minima zu verlassen.  

~\newline Während nun bereits ein gutes Grundverfahren vorliegt, gibt es noch weiter Möglichkeiten und Hyperparameter um die Optimierung zu beschleunigen:

\paragraph{Momentum} Eine einfache Verbesserung stellt die \textbf{Beschleunigung} dar: Sie verwaltet für jedes Gewicht einen Parameter, welcher bei einer Änderung angepasst wird. Werte die häufige bzw. große Änderungen erfahren erhalten eine größere Lernrate. Hiermit soll erreicht werden, dass ausschlaggebende Werte besser berücksichtigt sind und der Algorithmus im Gesamten schneller terminiert. 

~\newline Ergänzend zur Beschleunigung gibt es ebenfalls den \textbf{Verfall}. Dieser sorgt für eine regelmäßige Abnahme der Lernrate (d.h. bei jedem Durchlauf), und sollte mit der Beschleunigung abgestimmt werden. Der Verfall sollte nicht zu hoch gewählt werden, damit die Beschleunigungen bzw. die effektiven Lernraten stets bemerkbar bleiben. 
\paragraph{Adadelta} Eine weitere Verbesserung stellt der \textit{Adaptive Delta Gradientdescent} dar. Ziel dieser Variante ist es, die Lernrate immer passend zu wählen - in \textit{steilen} Gebieten \textit{große} Schritte zu vollziehen und in solchen nahe des Minimas \textit{kleinere}.

~\newline Adadelta addiert auf die bisherige Lernrate einen Wert abhängig von der aktuellen Änderung und den durchschnittlichen Gradienten-Verläufen über alle bisherigen Änderungen. Die konkrete Mathematische Ausarbeitung, sowie Beweise über die Terminierung und Verbesserungen der Trainingszeiten finden sich weiterführend im Whitepaper von Matthew D. Zeiler et. Al \cite{AdaDelta}.

Während die Beschleunigung eine \textit{First-Order}-Änderung darstellt, da nur der aktuelle Wert berücksichtigt wird, nimmt Adadelta auf den gesamten Verlauf Rücksicht und wird dahingehend als \textit{Second-Order} bezeichnet. 

~\newline Die Hauptstärke von Adadelta liegt darin, dass bewiesenermaßen eine gute Lernrate erzeugt wird ohne Input des Entwicklers. Dies spart viele Parameter im Gesamtprozess des Machine Learnings und reduziert somit ebenfalls Fehlerquellen.
\subsection{Bewertung der Klassifizierung}
Die Bewertung einer Klassifizierung erfolgt anhand dessen, wie viele Testdaten korrekt klassifiziert wurden. Dieses Verfahren eignet sich sowohl für die Binäre-, als auch für eine Multiklassen-Klassifikation
~\newline Die (relative) Genauigkeit ergibt sich als: 

\begin{equation}
	acc = \dfrac{\#\{t \in  TestSample | guess(t)==class(t)\}}{\#TestSample}
\end{equation}
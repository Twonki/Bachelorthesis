\section{Klassifizerung}
\label{sec:Klassifizierung}
Nach der linearen Regression soll nun die Klassifizierung vorgestellt werden. Hierfür werden zunächst allgemeine Ziele und ein Beispiel vorgestellt, anschließend wird die logistische Regression erläutert. 
 
\subsection{Konzept und Ziele von Klassifizierung}
Die Klassifizierung zählt zu den ältesten Anwendungen des Machine-Learning - Ein typisches Beispiel für (überwachte) Klassifizierung ist die Zuordnung einer E-Mail in \textit{Spam} oder \textit{Ham}.

~\newline Im Rahmen der Klassifizerung soll ein Modell erzeugt werden, das anhand der Eigenschaften einer E-Mail (z.B. dem Auftreten des Wortes \textit{Pharmacy} oder \textit{Sex}) feststellt, ob es sich um typische Spam-E-Mails handelt. 

~\newline Das Modell erzeugt einen Wahrscheinlichkeitswert, mit welchem die Klasse angenommen wird (bzw. im Umkehrschluss, wie wahrscheinlich das Gegenereignis ist) und \textit{rät} die entsprechende Klasse. 

~\newline Des Weiteren ist es möglich, eine sog. Multiklassen-Klassifizierung durchzuführen. Hierbei werden mehr als zwei Klassen betrachtet. 
\subsection{Logistische Regression}
Innerhalb der Logistischen Regression wird ein Modell erzeugt, welches einen Eigenschaftsvektor mit einem Gewichtsvektor multipliziert, und das Ergebnis über eine Aktivierungsfunktion in eine Wahrscheinlichkeit abbildet. 

~\newline Dieses Verfahren ermöglicht uns, anstatt der numerischen Zählung der \textit{Treffer}, die reale Wahrscheinlichkeit zu optimieren, und dahingehend unsere Gewichte \textit{smooth} zu justieren. 

~\newline Das Optimieren des Modells benötigt Trainingsdaten und eine \textbf{Optimierungsfunktion}. Mit jedem Satz der Trainingsdaten wird der Gewichtsvektor dahingehend angepasst, das die resultierende Wahrscheinlichkeit in Richtung des korrekten Ergebnisses verschoben wird. Das Maß in welchem die Gewichte Angepasst werden, wird über die Optimierungsfunktion ermittelt. 

~\newline Im Normalfall wird der Gewichtsvektor mit zufälligen Werten initialisiert. Es ist jedoch möglich, einen bereits bestehenden Vektor zu importieren. 

Ebenso ist es üblich, sowohl Eingabewerte, als auch den Gewichtsvektor zu normieren. Dies beschleunigt die Optimierung. 
\paragraph{Aktivierungsfunktion}
Bei der Aktivierungsfunktion handelt es sich um eine statistische Verteilungsfunktion. Sie bildet einen Wert auf eine Wahrscheinlichkeit ab. 

~\newline Als übliche Aktivierungsfunktionen werden $tanh$, die Sigmoid-Funktion oder die ReLu-Funktion gewählt. Diese besitzen besondere Eigenschaften innerhalb ihrer Ableitung, was eine Berechnung wesentlich erleichtert (\cite{stroetmann} S95ff \textit{6.3.1 The Sigmoid Function}) 

\paragraph{Optimierungsfunktion}
Die Optimierungsfunktion hilft, für eine (unbekannte) Funktion ein Extremum zu finden und wird konkret benötigt, um das Minimum der Fehlerfunktion zu erreichen.  

~\newline Im Rahmen des Machine-Learning verwaltet die Optimierungsfunktion ebenfalls Trainingsparameter, z.B. die Lernrate, eine Beschleunigungs- und Verfallslogik. Ebenso oft Bestandteil sind Funktionen, welche eine zufällige Verschiebung bewirken. Grund hierfür ist eine Schwäche der meisten Optimierungsfunktionen, sich auf ein lokales Extremum einzupendeln.

~\newline Übliche Aktivierungsfunktionen sind das \textit{Gradientenverfahren} sowie das \textit{Stochastische Gradientenverfahren}. 
\subsection{Bewertung der Klassifizierung}
Die Klassifizierung ist simpel dahingehend zu Bewerten, wieviele der Testdaten korrekt Klassifiziert wurden. 

~\newline Die (relative) Genauigkeit ergibt sich als: 

\begin{equation}
	acc = \dfrac{\#\{t \in  TestSample | guess(t)==class(t)\}}{\#TestSample}
\end{equation}
\chapter{Fazit}
\label{cha:Fazit} \label{cha:Schluss}
Im Rahmen der Arbeit haben sich verschiedene, übergreifende Ergebnisse herauskristallisiert welche nun abschließend aufbereitet werden.

~\newline In Bezug auf die Benutzung der Algorithmen gilt: \textit{\textbf{Easy to learn, hard to master}}. 

~\newline Moderne Frameworks und Bibliotheken ermöglichen es sehr einfach komplexe Modelle zu erstellen und zu benutzen. Sogar die Aufbereitung der Daten erfolgt einfach mithilfe von unterstützenden Frameworks oder wird sogar automatisch übernommen (z.B. die Normierung und Faktorisierung innerhalb von RevoScaler).

~\newline Dennoch liefern diese Modelle nicht unbedingt zufriedenstellende Ergebnisse: 

Entweder sind die Standard-Einstellungen nicht geeignet, oder die Aufbereitung der Daten produziert neue Fehler (z.B. wenn die Orte als ID übergeben werden, sind diese nicht faktorisiert). Auch entstehen häufig Fehler bei automatisch faktorisierten Daten, etwa weil innerhalb der Trainingsmenge eien Merkmalsausprägung gefehlt hat, welche in den Testdaten vorkommt und nicht korrekt auf den Eingabevektor übertragen werden kann. 

Fehler dieser Art zeigen sich erst bei der konkreten Anwendung des Modells auf separate Testdaten: Werden die \textit{korrumpierten} Trainingsdaten und Einstellungen in Standardeinstellungen verwendet, so zeigt das Modell bei zunehmenden Training ebenfalls Erfolg - es spiegelt nur nicht zwangsläufig ein \textit{gutes Modell} wieder.

~\newline Insofern sind die Frameworks und Standardeinstellungen zwar angenehm für den Einstieg, sollten allerdings für die Umsetzung eines Projektes genau inspiziert werden. Vor allem die Aufbereitung der Trainingsdaten und die Kontrolle des Modells sollte der Entwickler selbst übernehmen.  

~\newline Ein häufiger Satz, welcher v.A. in Internetforen auftritt, lautet \textit{\textbf{building neural networks is more art than science}}.

Diesen Satz sollte man nicht unreflektiert hinnehmen - viele Änderungen, Designentscheidungen und Trainingserfolge wurden bewusst durch die wissenschaftlichen Eigenschaften der einzelnen Komponenten erzielt. 

~\newline  Konkret zeigte sich dies bei der Benutzung der $tanh$-Funktion gegenüber der Sigmoid-Funktion in Abschnitt \ref{sec:RidesPred} welche Aufgrund ihrer Eigenschaften mit negativen Werten bessere Ergebnisse erzielte. 

~\newline Auch ist die Analyse des Datenbestandes, im Speziellen die Suche nach Anomalien, und das Design von Features im Bereich der Wissenschaft anzusiedeln. 

~\newline Zwar sind viele Fortschritte, z.B. das beheben von Over- und Underfitting, experimentell entstanden, dennoch können 
grundlegende Probleme auftreten, welche sich nicht (zeitnah) experimentell lösen lassen, im Rahmen dieser Arbeit konkret Abschnitt \ref{sec:PasPred} die Vorhersage des Passagieraufkommens und \ref{sec:RevPred} zur Schätzung des Umsatzes. Hier haben selbst umfangreiche Experimente keine Ergebnisse erzielt.

~\newline  Als letzter, großer Themenblock \textit{pro science} ist ebenfalls das Performancetuning zu nennen: 

Alle Hyperparameter der Optimierungsfunktion haben einen technischen Ursprung, und ein Verständnis der Algorithmen ist notwendig, um diese Parameter sinnvoll zu wählen. Zwar sind die Standardeinstellungen hier meistens hinreichend, allerdings können durch bewusste Anpassung ebenfalls markante Besserungen in der Trainingsgeschwindigkeit erzielt werden. 

So zeigten höhere Lern- und Verfalls-Parameter bei den weitestgehend homogenen Trinkgelddaten ein wesentlich schnelleres Terminieren das Trainings bei gleichbleibender Genauigkeit. 

Für andere Experimente, z.B. dem Fahrtenaufkommen, zeigten diese Einstellungen katastrophale Ergebnisse - die Daten waren einfach zu verschieden, um mit diesen Parametern behandelt zu werden. 

~\newline Große Teile sind allerdings rein experimentell - vor allem die optimale Größe der einzelnen hidden Layer zu finden, besteht im Wesentlichen aus Versuchen.

~\newline Insgesamt kann zwar jedes Problem rein experimentell gelöst werden, allerdings stellt dies oft mehr einen \textit{Glückstreffer} dar. 

Ein Verständnis der Algorithmen, ihrer Eigenschaften und der verwendeten Funktionen ermöglicht nicht nur eine bessere Ausgangslage für das Tuning, sondern ist eine Grundvoraussetzung für die Fehleranalyse und Problembehebung. 

~\newline Insofern ist der Ansatz, die Herangehensweise \textit{a priori} als Kunst zu bezeichnen nicht gerechtfertigt. Die Arbeit mit neuronalen Netzen entpuppte sich als gesunde Mischung aus Kunst und Wissenschaft. 

\paragraph{Zusammenfassung} ~\newline Zusammenfassend lässt sich anhand der Ergebnisse sagen, dass sich vor Aufgabenstellungen gut lösen lassen, welche \textit{nahe an einer Fahrt liegen}. Aussagen über aggregierte Daten benötigen besondere Aufbereitung und andere Herangehensweisen. Die Aufgaben, welche sich auf einzelne Fahrten bezogen, ließen sich im Allgemeinen einfach und mit guten Ergebnissen lösen.

Erstaunlicherweise decken sich die Versuche mit \textit{guten Ergebnissen} mit den Infos, die ein Taxifahrer in NY bereitstellen könnte: Wie viele Fahrten von einem Ort ausgehen, oder wie viel Trinkgeld seine Kunden geben. Andere Fragestellungen, wie der Gesamtumsatz über NY, oder wohin die Passagiere wollen, kann ein Taxifahrer ebenfalls schwer beantworten. 

~\newline Das Erstellen, Pflegen und Benutzen von neuronalen Netzen im SQL-Server gestaltet sich einfach, benötigt allerdings Zeit um Verbesserungen zu erarbeiten. Die Aufbereitung und Auswahl der Trainings- und Testdaten wird durch die Verwendung der Datenbank stark vereinfacht und ist wahrscheinlich einfacher als in anderen Umgebungen.

\paragraph{Ausblick}~\newline Innerhalb der Arbeit ließen sich zwei weiterführende Themen ausmachen: 

Der Fokus auf Alternativen bei der Umsatzprognose sowie die Übertragung und Verallgemeinerung der erarbeiteten Netze. Diese beiden Ansätze werden kurz ausgearbeitet. 

~\newline Die Umsatzprognose stellt nach wie vor den wichtigsten Use-Case für viele Unternehmen dar, und konnte ihm Rahmen dieser Arbeit mit neuronalen Netzen nicht gelöst werden. Im Zuge einer weiteren Arbeit kann der Hauptfokus auf der Umsatzprognose liegen, und verschiedene Formen der Vorhersage benutzt und verglichen werden um Ergebnisse zu erzielen. 

Möglichkeiten stellen hierbei klassische statistische Methoden dar, oder eine Vertiefung anderer technischer Ansätze wie etwa Timeseries-Datenbanken, welche auf zeitlich strukturierte Daten spezialisiert ist, oder Modellierung innerhalb einer Mehrdimensionalen-Datenbank, welche mit großen Faktoren innerhalb der Daten nativ umgeht.

~\newline Ein weiterer Punkt ist die Portierung der in dieser Arbeit erzeugten neuronalen Netze auf ähnliche Anwendungsgebiete, konkreter auf Taxidaten einer anderen Stadt. 

Hierbei zu lösen sind Schwierigkeiten bezüglich unterschiedlicher Eingabevektoren, vergleichbare Tests zu erzeugen sowie die technische Umsetzung innerhalb des SQL-Servers.  